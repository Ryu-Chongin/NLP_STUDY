{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_PROJECT_V1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6LQYFnVvAiVZNPP+1wrpY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryu-Chongin/NLP_STUDY/blob/main/LSTM_PROJECT_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b9A_d00o_BX"
      },
      "source": [
        "LSTM 모델을 이용한 NSMC 감정분석\r\n",
        "소스코드 참고 : https://wikidocs.net/44249 \r\n",
        "(출처 : Won Joon Yoo, Introduction to Deep Learning for Natural Language Processing, Wikidocs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYpsxnnXg1Bq"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A8LJ2qbgcoz"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "import urllib.request\r\n",
        "from konlpy.tag import Okt\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzGu7jDOhSly"
      },
      "source": [
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54BGx-4oglR9"
      },
      "source": [
        "# 판다스로 훈련셋과 테스트셋 데이터 로드\r\n",
        "train_data = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\r\n",
        "test_data = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t')\r\n",
        "\r\n",
        "print(train_data.shape)\r\n",
        "print(test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWXen26Mhx67"
      },
      "source": [
        "train_data['document'].nunique(), train_data['label'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o0kyaVmh1xe"
      },
      "source": [
        "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQErZKFjh7yz"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\r\n",
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gEoN1Omh-74"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i79QImFiEC6"
      },
      "source": [
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\r\n",
        "# 한글과 공백을 제외하고 모두 제거\r\n",
        "train_data[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVMVfuxjiIKE"
      },
      "source": [
        "train_data['document'].replace('', np.nan, inplace=True)\r\n",
        "print(train_data.isnull().sum())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmD2PHqfiMxL"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any')\r\n",
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwwUQS9niPzJ"
      },
      "source": [
        "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\r\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\r\n",
        "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\r\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거\r\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Z27nr3ibNk"
      },
      "source": [
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3WWGSgpideF"
      },
      "source": [
        "okt = Okt()\r\n",
        "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s5Tfn9EijT2"
      },
      "source": [
        "X_train = []\r\n",
        "for sentence in train_data['document']:\r\n",
        "    temp_X = []\r\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\r\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\r\n",
        "    X_train.append(temp_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MB-1NhS37tJ"
      },
      "source": [
        "X_test = []\r\n",
        "for sentence in test_data['document']:\r\n",
        "    temp_X = []\r\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\r\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\r\n",
        "    X_test.append(temp_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvBgfaYFimSI"
      },
      "source": [
        "print(X_train[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP--thLris4y"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTTmWcYYiuh9"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0yAKU21ixS_"
      },
      "source": [
        "threshold = 3\r\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\r\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\r\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\r\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\r\n",
        "\r\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\r\n",
        "for key, value in tokenizer.word_counts.items():\r\n",
        "    total_freq = total_freq + value\r\n",
        "\r\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\r\n",
        "    if(value < threshold):\r\n",
        "        rare_cnt = rare_cnt + 1\r\n",
        "        rare_freq = rare_freq + value\r\n",
        "\r\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\r\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\r\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\r\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N2eu_KFiz-G"
      },
      "source": [
        "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\r\n",
        "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\r\n",
        "vocab_size = total_cnt - rare_cnt + 2\r\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS-DrkTRi13U"
      },
      "source": [
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\r\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q9uOrEni7fz"
      },
      "source": [
        "print(X_train[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5K2vlBki5aC"
      },
      "source": [
        "y_train = np.array(train_data['label'])\r\n",
        "y_test = np.array(test_data['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4GUz0ji-qw"
      },
      "source": [
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEajiUZMjDGq"
      },
      "source": [
        "# 빈 샘플들을 제거\r\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\r\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\r\n",
        "print(len(X_train))\r\n",
        "print(len(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FslrKTzDjFJm"
      },
      "source": [
        "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\r\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\r\n",
        "plt.hist([len(s) for s in X_train], bins=50)\r\n",
        "plt.xlabel('length of samples')\r\n",
        "plt.ylabel('number of samples')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhM7YifglfPN"
      },
      "source": [
        "def below_threshold_len(max_len, nested_list):\r\n",
        "  cnt = 0\r\n",
        "  for s in nested_list:\r\n",
        "    if(len(s) <= max_len):\r\n",
        "        cnt = cnt + 1\r\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqwZ7Upnlaro"
      },
      "source": [
        "max_len = 30\r\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NJJL88ujKEo"
      },
      "source": [
        "X_train = pad_sequences(X_train, maxlen = max_len)\r\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHnxOtk2jPPY"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-PsVYAxjQnX"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, 100))\r\n",
        "model.add(LSTM(128))\r\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOIAd-LxjSh2"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5k-dYp-jUO7"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\r\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C8XwqHho8OP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVL1G_ZNjYCg"
      },
      "source": [
        "리뷰 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7_F9pN9jXlw"
      },
      "source": [
        "def sentiment_predict(new_sentence):\r\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\r\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\r\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\r\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\r\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\r\n",
        "  if(score > 0.5):\r\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\r\n",
        "  else:\r\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RvSbUicp5KJ"
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\r\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUSvHnJJjd4j"
      },
      "source": [
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvUtPin9qD0K"
      },
      "source": [
        "결과값 확인을 위한 출력부 수정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmNyVs5OqCYx"
      },
      "source": [
        "def sentiment_predict2(new_sentence):\r\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\r\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\r\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\r\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\r\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\r\n",
        "  if(score > 0.5):\r\n",
        "    #print(1)\r\n",
        "    #1\r\n",
        "    return 1\r\n",
        "  else:\r\n",
        "    return 0\r\n",
        "    #print(0)\r\n",
        "    #0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLRmIlV_qQKU"
      },
      "source": [
        "sentiment_predict2('이 영화 별론데')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8AZMQDwqgD9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x3kFC55qmtc"
      },
      "source": [
        "filename = '/content/drive/MyDrive/test/My_drive/ko_data.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABln-CxFqqFO"
      },
      "source": [
        "data = pd.read_csv(filename ,sep=',',engine='python',encoding='CP949')\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3B0HUPCuGK0"
      },
      "source": [
        "캐글용 TEST DATA 입력해서 결과값 생성하기."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNNg92kcunmd"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMRDd52gqwfE"
      },
      "source": [
        "i = 0\r\n",
        "\r\n",
        "pred_result =[]\r\n",
        "\r\n",
        "for i in range(data.shape[0]):\r\n",
        "#for i in range(100):\r\n",
        "\r\n",
        "    test_single_sen = data.Sentence[i]\r\n",
        "    #print(test_single_sen)\r\n",
        "    logits = sentiment_predict2(test_single_sen)\r\n",
        "    #print(\"!!!!\" , logits)\r\n",
        "\r\n",
        "    #print(logits)\r\n",
        "    #print(np.argmax(logits))\r\n",
        "    if  i % 500 == 0 :\r\n",
        "        print( i, \"번째 예측중....\")\r\n",
        "    #result = (i, np.argmax(logits))\r\n",
        "    result = [i, logits] \r\n",
        "\r\n",
        "    #print(i, result , data.Sentence[i])  \r\n",
        "    \r\n",
        "    pred_result.append(result)\r\n",
        "\r\n",
        "df = pd.DataFrame(pred_result)\r\n",
        "\r\n",
        "print(\"예측 종료...!!!\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azQdG4iUrZ8h"
      },
      "source": [
        "lstm_test_csv = df.to_csv('lstm_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}