{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_PROJECT_V2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPD2teNVUjoGY6/OAITViie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryu-Chongin/NLP_STUDY/blob/main/LSTM_PROJECT_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b9A_d00o_BX"
      },
      "source": [
        "LSTM 모델을 이용한 NSMC 감정분석\r\n",
        "소스코드 참고 : 막돼먹은 엔지니어의 머신러닝 개발\r\n",
        "딥러닝 모델링 & 강화학습 알고리듬 \r\n",
        "(출처 : https://skettee.github.io/post/long_short_term_memory/#%ED%98%95%ED%83%9C%EC%86%8C-%EB%B6%84%EC%84%9D%EA%B8%B0-%EC%84%A4%EC%B9%98)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYpsxnnXg1Bq"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A8LJ2qbgcoz"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "import urllib.request\r\n",
        "from konlpy.tag import Okt\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIG8g0CF3DdB"
      },
      "source": [
        "from tensorflow.keras.utils import get_file\r\n",
        "\r\n",
        "train_fname = 'ratings_train.tsv'\r\n",
        "test_fname = 'ratings_test.tsv'\r\n",
        "train_origin = 'https://raw.github.com/e9t/nsmc/master/ratings_train.txt'\r\n",
        "test_origin = 'https://raw.github.com/e9t/nsmc/master/ratings_test.txt'\r\n",
        "\r\n",
        "train_path = get_file(train_fname, train_origin)\r\n",
        "test_path = get_file(test_fname, test_origin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rHfq6VF3HzH"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "train_df = pd.read_csv(train_path, sep='\\t') # tsv file\r\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKGrgCJ93LDa"
      },
      "source": [
        "test_df = pd.read_csv(test_path, sep='\\t') # tsv file\r\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scbAV4XH3bkP"
      },
      "source": [
        "train_df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGcMg0GJ3e5h"
      },
      "source": [
        "train_df = train_df.dropna(axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMgvJglA3lWi"
      },
      "source": [
        "test_df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EKhgax_3pXh"
      },
      "source": [
        "test_df = test_df.dropna(axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzDOsD3X3uqj"
      },
      "source": [
        "print('Train data shape: ', train_df.shape)\r\n",
        "n_lebel = len(train_df[train_df.label == 0])\r\n",
        "print('Label 0 in Train data: {} ({:.1f}%)'.format(n_lebel, n_lebel*100/len(train_df)))\r\n",
        "n_lebel = len(train_df[train_df.label == 1])\r\n",
        "print('Label 1 in Train data: {} ({:.1f}%)'.format(n_lebel, n_lebel*100/len(train_df)))\r\n",
        "\r\n",
        "print('\\nTest data shape: ', test_df.shape)\r\n",
        "n_lebel = len(test_df[test_df.label == 0])\r\n",
        "print('Label 0 in Test data: {} ({:.1f}%)'.format(n_lebel, n_lebel*100/len(test_df)))\r\n",
        "n_lebel = len(test_df[test_df.label == 1])\r\n",
        "print('Label 1 in Test data: {} ({:.1f}%)'.format(n_lebel, n_lebel*100/len(test_df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNixXq3k3zQM"
      },
      "source": [
        "train_df = train_df[['document', 'label']]\r\n",
        "test_df = test_df[['document', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xJfUpoo32my"
      },
      "source": [
        "%pip install PyKomoran"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gd1hp_F36Hn"
      },
      "source": [
        "from PyKomoran import *\r\n",
        "\r\n",
        "corpus = \"① 대한민국은 민주공화국이다.\"\r\n",
        "komoran = Komoran(\"STABLE\")\r\n",
        "komoran.get_plain_text(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqzNJU9X391U"
      },
      "source": [
        "stop_pos_tags =  ['IC', 'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', \r\n",
        "                   'EF', 'ETN', 'ETM', 'XSA', 'SF', 'SP', 'SS', 'SE', 'SO', 'SL', 'SH', \r\n",
        "                   'SW', 'NF', 'NV', 'SN', 'NA']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kc9Id0X4Adk"
      },
      "source": [
        "def tokenize(corpus, stop_pos_tags):\r\n",
        "    result = []\r\n",
        "    pairs = komoran.get_list(corpus)\r\n",
        "    for pair in pairs:\r\n",
        "        morph = pair.get_first()\r\n",
        "        pos = pair.get_second()\r\n",
        "        if pos not in stop_pos_tags:\r\n",
        "            if pos in ['VV', 'VA', 'VX', 'VCP', 'VCN']:\r\n",
        "                morph = morph + '다'\r\n",
        "            result.append(morph)\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc-XbKPL4EE8"
      },
      "source": [
        "tokens_list = []\r\n",
        "\r\n",
        "for i in range(len(train_df['document'])):\r\n",
        "    tokens_list.append(tokenize(train_df['document'][i], stop_pos_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l52vipv4IpV"
      },
      "source": [
        "train_df['tokens'] = tokens_list\r\n",
        "\r\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fmYWFRG-EgV"
      },
      "source": [
        "train_df = train_df[train_df['tokens'].str.len() > 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLGNNUm5-IBQ"
      },
      "source": [
        "tokens_list = []\r\n",
        "\r\n",
        "for i in range(len(test_df['document'])):\r\n",
        "    tokens_list.append(tokenize(test_df['document'][i], stop_pos_tags))\r\n",
        "test_df['tokens'] = tokens_list\r\n",
        "\r\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ZAW6f4_2wa"
      },
      "source": [
        "test_df = test_df[test_df['tokens'].str.len() > 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzgWPtAP_60c"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "import os\r\n",
        "import pickle\r\n",
        "\r\n",
        "tokenizer_name = 'keras_naver_review_tokenizer.pickle'\r\n",
        "save_path = os.path.join(os.getcwd(), tokenizer_name)\r\n",
        "\r\n",
        "max_words = 35000\r\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token = True)\r\n",
        "tokenizer.fit_on_texts(train_df.tokens)\r\n",
        "train_df.tokens = tokenizer.texts_to_sequences(train_df.tokens)\r\n",
        "test_df.tokens = tokenizer.texts_to_sequences(test_df.tokens)\r\n",
        "\r\n",
        "with open(save_path, 'wb') as f:\r\n",
        "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT7JomZG__fR"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzEgwfiEAIKr"
      },
      "source": [
        "X_train = train_df.tokens\r\n",
        "Y_train = train_df.label\r\n",
        "\r\n",
        "X_test = test_df.tokens\r\n",
        "Y_test = test_df.label\r\n",
        "\r\n",
        "print('X_train shape: ', X_train.shape)\r\n",
        "print('Y_train shape: ', Y_train.shape)\r\n",
        "print('\\nX_test shape: ', X_test.shape)\r\n",
        "print('Y_test shape: ', Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKxCXfbRALrt"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "max_len=40\r\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\r\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\r\n",
        "\r\n",
        "print('X_train shape: ', X_train.shape)\r\n",
        "print('X_test shape: ', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAyC8U01APZS"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "encoder = LabelEncoder()\r\n",
        "# Train\r\n",
        "batch_size = Y_train.shape[0]\r\n",
        "input_dim = 1\r\n",
        "Y_train = encoder.fit_transform(Y_train) # Labeling\r\n",
        "Y_train = np.reshape(Y_train, (batch_size, input_dim)) # Reshape\r\n",
        "# Test\r\n",
        "batch_size = Y_test.shape[0]\r\n",
        "Y_test = encoder.transform(Y_test) # Labeling\r\n",
        "Y_test = np.reshape(Y_test, (batch_size, input_dim)) # Reshape\r\n",
        "\r\n",
        "print(Y_train.shape)\r\n",
        "print(Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXKRNL1yAUS7"
      },
      "source": [
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(max_words, 128))\r\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl4IR2c2Adwq"
      },
      "source": [
        "hist = model.fit(X_train, Y_train, batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpvsEc9PHkcO"
      },
      "source": [
        "loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\r\n",
        "\r\n",
        "print('Test loss:', loss)\r\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC8ebig_IGQn"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "save_dir = os.getcwd()\r\n",
        "model_name = 'keras_naver_review_trained_model.h5'\r\n",
        "\r\n",
        "# Save model and weights\r\n",
        "model_path = os.path.join(save_dir, model_name)\r\n",
        "model.save(model_path)\r\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5011kAKhIK8y"
      },
      "source": [
        "from  tensorflow.keras.models import load_model\r\n",
        "import os\r\n",
        "import pickle\r\n",
        "\r\n",
        "def load_tokenizer(path):\r\n",
        "    with open(path, 'rb') as f:\r\n",
        "        tokenizer = pickle.load(f)\r\n",
        "    return tokenizer\r\n",
        "\r\n",
        "load_dir = os.getcwd()\r\n",
        "model_name = 'keras_naver_review_trained_model.h5'\r\n",
        "tokenizer_name = 'keras_naver_review_tokenizer.pickle'\r\n",
        "model_path = os.path.join(load_dir, model_name)\r\n",
        "tokenizer_path = os.path.join(load_dir, tokenizer_name)\r\n",
        "\r\n",
        "model = load_model(model_path)\r\n",
        "tokenizer = load_tokenizer(tokenizer_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGNqFZBcRSpN"
      },
      "source": [
        "import numpy as np\r\n",
        "from PyKomoran import *\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "max_len=40\r\n",
        "komoran = Komoran(\"STABLE\")\r\n",
        "stop_pos_tags =  ['IC', 'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', \r\n",
        "                   'EF', 'ETN', 'ETM', 'XSA', 'SF', 'SP', 'SS', 'SE', 'SO', 'SL', 'SH', \r\n",
        "                   'SW', 'NF', 'NV', 'SN', 'NA']\r\n",
        "\r\n",
        "def tokenize(corpus, stop_pos_tags):\r\n",
        "    result = []\r\n",
        "    pairs = komoran.get_list(corpus)\r\n",
        "    for pair in pairs:\r\n",
        "        morph = pair.get_first()\r\n",
        "        pos = pair.get_second()\r\n",
        "        if pos not in stop_pos_tags:\r\n",
        "            if pos in ['VV', 'VA', 'VX', 'VCP', 'VCN']:\r\n",
        "                morph = morph + '다'\r\n",
        "            result.append(morph)\r\n",
        "    return result\r\n",
        "\r\n",
        "def predict_sentiment(text, model):\r\n",
        "    tokens = []\r\n",
        "    tokens.append(tokenize(text, stop_pos_tags))\r\n",
        "    tokens = tokenizer.texts_to_sequences(tokens)\r\n",
        "    x_test = pad_sequences(tokens, maxlen=max_len)\r\n",
        "    predict = model.predict(x_test)\r\n",
        "    if predict[0] > 0.5:\r\n",
        "        return 1\r\n",
        "    else:\r\n",
        "        return 0\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBgcQs5dR-k0"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Pb2UneRsq1"
      },
      "source": [
        "filename = '/content/drive/MyDrive/test/My_drive/ko_data.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly3us0F9Rwy4"
      },
      "source": [
        "data = pd.read_csv(filename ,sep=',',engine='python',encoding='CP949')\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JgVa4H7SMUm"
      },
      "source": [
        "i = 0\r\n",
        "\r\n",
        "pred_result =[]\r\n",
        "\r\n",
        "for i in range(data.shape[0]):\r\n",
        "#for i in range(100):\r\n",
        "\r\n",
        "    test_single_sen = data.Sentence[i]\r\n",
        "    #print(test_single_sen)\r\n",
        "    logits = predict_sentiment(test_single_sen , model)\r\n",
        "    #print(\"!!!!\" , logits)\r\n",
        "\r\n",
        "    #print(logits)\r\n",
        "    #print(np.argmax(logits))\r\n",
        "    if  i % 500 == 0 :\r\n",
        "        print( i, \"번째 예측중....\")\r\n",
        "    #result = (i, np.argmax(logits))\r\n",
        "    result = [i, logits] \r\n",
        "\r\n",
        "    #print(i, result , data.Sentence[i])  \r\n",
        "    \r\n",
        "    pred_result.append(result)\r\n",
        "\r\n",
        "df = pd.DataFrame(pred_result)\r\n",
        "\r\n",
        "print(\"예측 종료...!!!\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrhzNhuESWnE"
      },
      "source": [
        "lstm_test_v2_csv = df.to_csv('lstm_test_v2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}